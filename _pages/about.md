---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


<span class='anchor' id='about-me'></span>
# 🤔 About-me

I earned my Master’s degree in Artificial Intelligence from [Tsinghua University](https://www.tsinghua.edu.cn/en/) <img src='./images/thu.png' style='width: 2em;'>, where I conducted research under the supervision of [Prof. Haoqian Wang](https://www.sigs.tsinghua.edu.cn/whq_en/main.htm) and collaborated closely with [Prof. Yebin Liu](https://www.liuyebin.com/) on 3DV & Human Avatar Reconstruction. Prior to this, I completed my B.Eng. in Measurement and Control Technology & Instruments at [Southeast University](https://www.seu.edu.cn/english/) <img src='./images/seu.png' style='width: 2em;'>. During my graduate studies, I also had the privilege of visiting [Harvard University](https://www.harvard.edu/) <img src='./images/seas.png' style='width: 1.2em;'> as a research intern, working with [Prof. Hanspeter Pfister](https://vcg.seas.harvard.edu/people/hanspeter-pfister) on computer graphics.

I am currently an Researcher at ByteDance Seed <img src='./images/bytedance.png' style='width: 6em;'>, focusing on cutting-edge challenges in <b>perception, generation, and world model</b>. 

<a href='https://scholar.google.com/citations?user=ngEXyLkAAAAJ&hl=en&authuser=1'><img src='./images/scholar_badge.svg' alt='Google Scholar Citations' style='height: 1.5em; vertical-align: middle;'></a>

I am actively recruiting research interns to collaborate on:

📌 3D Scene Perception.

📌 3D Content Creation.

📌 World Model.

If you are seeking any form of academic cooperation, please feel free to email me at [qinminghan1999@gmail.com](mailto:qinminghan1999@gmail.com).

# 🔥 News
- *2025.09*: &nbsp;🎉🎉 3 paper accepted to NeurIPS 2025 !!!
- *2025.07*: &nbsp;🎉🎉 1 paper accepted to ACM MM 2025 !!!
- *2025.06*: &nbsp;🎉🎉 2 paper accepted to ICCV 2025 !!!
- *2025.06*: &nbsp;🎉🎉 NOVA3D has been selected as [ICME 2025 <font color="red"><b>Bestpaper Candidate</b></font>](https://2025.ieeeicme.org/awards/)!!!
- *2025.02*: &nbsp;🎉🎉 2 paper accepted to CVPR 2025 !!!
<details>
<summary>More News</summary>

- *2024.09*: &nbsp;🎉🎉 1 paper accepted to NeurIPS 2024 !!!
- *2024.07*: &nbsp;🎉🎉 1 paper accepted to ACM MM 2024 !!!
- *2024.02*: &nbsp;🎉🎉 2 paper accepted to ECCV 2024 !!!
- *2024.02*: &nbsp;🎉🎉 LangSplat has been selected as CVPR 2024 <font color="red"><b>Highlight</b></font> !!!
- *2024.02*: &nbsp;🎉🎉 1 paper accepted to CVPR 2024 !!!
- *2023.11*: &nbsp;🎉🎉 1 paper accepted to AAAI 2024 !!!
</details>

# 📝 Publications

<div style="text-align: center; margin: 10px 0;">
  <a href="#multimodal-3d-perception" style="margin: 0 5px; padding: 5px 10px; border: 1px solid #ddd; border-radius: 15px; background: #f8f8f8; text-decoration: none; color: #333; font-size: 0.9em;">Vision-Language 3D Perception</a>
  <a href="#generation-model" style="margin: 0 5px; padding: 5px 10px; border: 1px solid #ddd; border-radius: 15px; background: #f8f8f8; text-decoration: none; color: #333; font-size: 0.9em;">Generation Model</a>
  <a href="#digital-human" style="margin: 0 5px; padding: 5px 10px; border: 1px solid #ddd; border-radius: 15px; background: #f8f8f8; text-decoration: none; color: #333; font-size: 0.9em;">Digital Human</a>
  <a href="#3d-reconstruction" style="margin: 0 5px; padding: 5px 10px; border: 1px solid #ddd; border-radius: 15px; background: #f8f8f8; text-decoration: none; color: #333; font-size: 0.9em;">3D Reconstruction</a>
</div>

<h2 id="multimodal-3d-perception">Vision-Language 3D Perception</h2> 
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2024 Highlight</div><img src='images/langsplat.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[LangSplat: 3D Language Gaussian Splatting](https://arxiv.org/pdf/2312.16084.pdf)

<b>Minghan Qin\*</b>, Wanhua Li\*†, Jiawei Zhou\*, Haoqian Wang†, Hanspeter Pfister

[![Website](https://img.shields.io/badge/Web-Project-green)](https://langsplat.github.io/) [![](https://img.shields.io/github/stars/minghanqin/Langsplat?style=flat-square&label=GitHub%20Star)](https://github.com/minghanqin/LangSplat) <strong><span class='show_paper_citations' data='ngEXyLkAAAAJ:u-x6o8ySG0sC'></span></strong>

<a href="https://trendshift.io/repositories/6471" target="_blank"><img src="https://trendshift.io/api/badge/repositories/6471" alt="minghanqin%2FLangSplat | Trendshift" style="width: 250px; height: 55px;" width="250" height="55"/></a> 
  - We introduces LangSplat, which constructs a 3D language field that enables precise and efficient open-vocabulary querying within 3D spaces.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2025</div><img src='images/langsplatv2.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[LangSplatV2: High-dimensional 3D Language Gaussian Splatting with 450+ FPS](https://arxiv.org/abs/2507.07136)

Wanhua Li\*, Yujie Zhao\*, <b>Minghan Qin\*</b>, Yang Liu, Yuanhao Cai, Chuang Gan, Hanspeter Pfister

[![Website](https://img.shields.io/badge/Web-Project-green)](https://langsplat-v2.github.io/) [![](https://img.shields.io/github/stars/ZhaoYujie2002/LangSplatV2?style=flat-square&label=GitHub%20Star)](https://github.com/ZhaoYujie2002/LangSplatV2)

  - We present LangSplatV2, which achieves high-dimensional feature splatting at 476.2 FPS and 3D open-vocabulary text querying at 384.6 FPS for high-resolution images.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2025</div><img src='images/4d_LangSplat.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[4D LangSplat: 4D Language Gaussian Splatting via Multimodal Large Language Models](https://arxiv.org/pdf/2503.10437)

Wanhua Li\*, Renping Zhou\*, Jiawei Zhou, Yingwei Song, Johannes Herter, <b>Minghan Qin</b>, Gao Huang, Hanspeter Pfister

[![Website](https://img.shields.io/badge/Web-Project-green)](https://4d-langsplat.github.io/)

- We present 4D LangSplat, an approach to constructing a dynamic 4D language field in evolving scenes, leveraging Multimodal Large Language Models.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Arxiv</div><img src='images/langsurf.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[LangSurf: Language-Embedded Surface Gaussians for 3D Scene Understanding](https://arxiv.org/pdf/2412.17635)

Hao Li\*, <b>Minghan Qin\*†</b>, Zhengyu Zou\*, Diqi He, Bohan Li, Bingquan Dai, Dingwen Zhang†, Junwei Han

[![Website](https://img.shields.io/badge/Web-Project-green)](https://langsurf.github.io/)

- We propose LangSurf, a model that aligns language features with object surfaces to enhance 3D scene understanding
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM MM 2025</div><img src='images/slgaussian.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[SLGaussian: Fast Language Gaussian Splatting in Sparse Views](https://arxiv.org/pdf/2412.08331)

Kangjie Chen\*, Bingquan Dai\*, <b>Minghan Qin</b>, Dongbin Zhang, Peihao Li, Yingshuang Zou, Haoqian Wang† 

[![Website](https://img.shields.io/badge/Web-Project-green)](https://chenkangjie1123.github.io/SLGaussian.github.io/)

- We propose SLGaussian, a feed-forward method for constructing 3D semantic fields from sparse viewpoints, allowing direct inference of 3DGS-based scenes.
</div>
</div>

<h2 id="generation-model">Generation Model</h2>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2025</div><img src='images/meshcoder.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds](https://arxiv.org/pdf/2508.14879)

Bingquan Dai\*, Li Ray Luo\*, Qihong Tang, Jie Wang, Xinyu Lian, Hao Xu, <b>Minghan Qin</b>, Xudong Xu, Bo Dai, Haoqian Wang†, Zhaoyang Lyu†, Jiangmiao Pang

[![Website](https://img.shields.io/badge/Web-Project-green)](https://daibingquan.github.io/MeshCoder/#)

-  MeshCoder generates structured mesh code from 3D point clouds, enhancing shape editing and understanding.

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2025</div><img src='images/vap.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[VAP: Precise Action-to-Video Generation through Visual Action Prompts](https://drive.google.com/file/d/16NowbdeyUlojIUr2huKnaspHCE7yq9Ll/view)

Yuang Wang, Chao Wen, Haoyu Guo, Sida Peng, <b>Minghan Qin</b>, Hujun Bao, Xiaowei Zhou, Ruizhen Hu

[![Website](https://img.shields.io/badge/Web-Project-green)](https://zju3dv.github.io/VAP/)

-  VAP harnesses subject renderings as action proxies for interactive video generation, striking an balance between precision and generality in action representation.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICME 2025, Best Paper Award Candidate</div><img src='images/nova3d.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[NOVA3D: Normal Aligned Video Diffusion Model for Single Image to 3D Generation](https://arxiv.org/pdf/2506.07698)

Yuxiao Yang, Peihao Li, Yuhong Zhang, Junzhe Lu, Xianglong He, <b>Minghan Qin</b>, Weitao Wang, Haoqian Wang†
-  NOVA3D unleashes geometric 3D prior from a video diffusion model to generate high-quality textured meshes from input image.
</div>
</div>

<h2 id="digital-human">Digital Human</h2>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICCV 2025</div><img src='images/GUAVA.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[GUAVA: Generalizable Upper Body 3D Gaussian Avatar](https://arxiv.org/pdf/2505.03351)

Dongbin Zhang, Yunfei Liu, Lijian Lin, Ye Zhu, Yang Li, <b>Minghan Qin</b>, Yu Li, Haoqian Wang

[![Website](https://img.shields.io/badge/Web-Project-green)](https://eastbeanzhang.github.io/GUAVA/)

- For each single image with a tracked pose, GUAVA can reconstruct a 3D upper-body Gaussian avatar via feed-forward inference within sub-second time, enabling real-time expressive animation and novel view synthesis at 512✖️512 resolution.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2025</div><img src='images/hravatar.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[HRAvatar: High-Quality and Relightable Gaussian Head Avatar](https://eastbeanzhang.github.io/HRAvatar/static/paper/HRAvatar.pdf)

Dongbin Zhang, Yunfei Liu, Lijian Lin, Ye Zhu, Kangjie Chen, <b>Minghan Qin</b>, Yu Li†, Haoqian Wang†

[![Website](https://img.shields.io/badge/Web-Project-green)](https://eastbeanzhang.github.io/HRAvatar/)

- With monocular video input, HRAvatar reconstructs a high-quality, animatable 3D head avatar that enables realistic relighting effects and simple material editing.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM MM 2024</div><img src='images/animatablegaussian.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Animatable 3d gaussian: Fast and high-quality reconstruction of multiple human avatars](https://arxiv.org/pdf/2311.16482)

Yang Liu*, Xiang Huang*, <b>Minghan Qin</b>, Qinwei Lin, Haoqian Wang (* indicates equal contribution)

[![Website](https://img.shields.io/badge/Web-Project-green)](https://jimmyyliu.github.io/Animatable-3D-Gaussian/) [![](https://img.shields.io/github/stars/jimmyYliu/Animatable-3d-Gaussian?style=flat-square&label=GitHub%20Star)](https://github.com/jimmyYliu/Animatable-3d-Gaussian)
- We propose Animatable 3D Gaussian, a novel neural representation for fast and high-fidelity reconstruction of multiple animatable human avatars, which can animate and render the model at interactive rate.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">AAAI 2024</div><img src='images/avatarsve.jpg' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[High-Fidelity 3D Head Avatars Reconstruction through Spatially-Varying Expression Conditioned Neural Radiance Field](https://arxiv.org/abs/2310.06275)

<b>Minghan Qin\*</b>, Yifan Liu\*, Yuelang Xu, Xiaochen Zhao, Yebin Liu†, Haoqian Wang†

[![Website](https://img.shields.io/badge/Web-Project-green)](https://minghanqin.github.io/AvatarSVE/) -  We introduce a novel Spatially-Varying Expression (SVE) conditioning, encompassing both spatial positional features and global expression information.
</div>
</div>

<h2 id="3d-reconstruction">3D Reconstruction</h2>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2025</div><img src='images/QAC.png' alt="sym" width="80%"></div></div>
<div class='paper-box-text' markdown="1">

[Quantifying and Alleviating Co-Adaptation in Sparse-View 3D Gaussian Splatting](https://arxiv.org/pdf/2405.15125)

Kangjie Chen, Yingji Zhong, Zhihao Li, Jiaqi Lin, Youyu Chen, <b>Minghan Qin</b>, Haoqian Wang†

[![Website](https://img.shields.io/badge/Web-Project-green)](https://chenkangjie1123.github.io/Co-Adaptation-3DGS/) [![](https://img.shields.io/github/stars/chenkangjie1123/Co-Adaptation-of-3DGS?style=flat-square&label=GitHub%20Star)](https://github.com/chenkangjie1123/Co-Adaptation-of-3DGS/)
- This paper introduces the concept of co-adaptation in 3D Gaussian Splatting, analyzes its impact on rendering artifacts, and proposes strategies (Dropout Regularization & Opacity Noise Injection) to reduce it.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2024</div><img src='images/hdr_gs.gif' alt="sym" width="80%"></div></div>
<div class='paper-box-text' markdown="1">

[HDR-GS: Efficient High Dynamic Range Novel View Synthesis at 1000x Speed via Gaussian Splatting](https://arxiv.org/pdf/2405.15125)

Yuanhao Cai , Zihao Xiao, Yixun Liang, <b>Minghan Qin</b>, Yulun Zhang, Xiaokang Yang, Yaoyao Liu, Alan Yuille

[![Website](https://img.shields.io/badge/Web-Project-green)](https://github.com/caiyuanhao1998/HDR-GS) [![](https://img.shields.io/github/stars/caiyuanhao1998/HDR-GS?style=flat-square&label=GitHub%20Star)](https://github.com/caiyuanhao1998/HDR-GS)
- This paper presents HDR-GS, a novel 3D Gaussian splatting-based method for high dynamic range imaging that achieves 1000x speedup over existing methods.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2024</div><img src='images/GS-W.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Gaussian in the Wild: 3D Gaussian Splatting for Unconstrained Image Collections](https://arxiv.org/pdf/2403.15704.pdf)

Dongbin Zhang\*, Chuming Wang\*, Weitao Wang, Peihao Li, <b>Minghan Qin</b>, Haoqian Wang†

[![Website](https://img.shields.io/badge/Web-Project-green)](https://eastbeanzhang.github.io/GS-W/) [![](https://img.shields.io/github/stars/eastbeanzhang/Gaussian-Wild?style=flat-square&label=GitHub%20Star)](https://github.com/EastbeanZhang/Gaussian-Wild)
-  We utilize 3D Gaussian Splatting with introduced separated intrinsic and dynamic appearance to reconstruct scenes from uncontrolled images, achieving high-quality results and a 1000 × rendering speed increase.
</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ECCV 2024</div><img src='images/coders.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Category-level Object Detection, Pose Estimation and Reconstruction from Stereo Images](https://arxiv.org/abs/2407.06984)

Chuanrui Zhang\*, Yonggen Ling\*†, Minglei Lu, <b>Minghan Qin</b>, Haoqian Wang†

[![Website](https://img.shields.io/badge/Web-Project-green)](https://xingyoujun.github.io/coders) [![Datasets](https://img.shields.io/badge/Web-Datasets-blue)](https://huggingface.co/datasets/xingyoujun/ss3d)
-  We present CODERS, a one-stage approach for Category-level Object Detection, pose Estimation and Reconstruction from Stereo images.
</div>
</div>

# 🎖 Honors and Awards
- Winner of [CVPR 2025 Workshop on Photorealistic 3D Head Avatars (P3HA) Train Viewpoint Track](https://kaldir.vc.in.tum.de/nersemble_benchmark/benchmark/mono_flame_avatar?sortby=psnr&track=seen_viewpoints).
- Scholarship, [Tsinghua University](https://www.tsinghua.edu.cn/en/), 2023. 
- National 1st Award, [the 10th BD-CASTIC](https://ins.seu.edu.cn/2019/0410/c45116a435789/page.psp), 2019.

# 💻 Research Experience

- *2023 - 2024*, Harvard University - VCG Lab - Computer Vision Group. I spent a good time with [Wanhua Li](https://li-wanhua.github.io/) and [Prof. Hanspeter Pfister](https://vcg.seas.harvard.edu/people/hanspeter-pfister).

# 💁 Academic Service

Reviewers of: CVPR, ECCV, ICCV, NeurIPS, SIGGRAPH, ACM MM, AAAI, 3DV, etc.
